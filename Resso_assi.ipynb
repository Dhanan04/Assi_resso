{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1266fdc0-db62-4a93-b49e-860b4353792d",
   "metadata": {},
   "source": [
    "Q1. ans\n",
    "\n",
    "Ridge Regression, also known as L2 regularization, is a linear regression technique that introduces a regularization term to the ordinary least squares (OLS) regression cost function. It is used to address the problems of multicollinearity (high correlation between predictor variables) and overfitting in linear regression models.\n",
    "\n",
    "Ridge Regression differs from ordinary least squares regression by adding a regularization term based on the squared magnitudes of the coefficients. This regularization term helps in handling multicollinearity, reducing model complexity, and providing more stable coefficient estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e757c7-ebc3-4ef4-8026-485597c681f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fb3b461-ebf6-43cc-b3f0-1fe82d9e3d57",
   "metadata": {},
   "source": [
    "Q2. ans\n",
    "\n",
    "Ridge Regression assumes that the relationship between the dependent variable and the independent variables is linear. The model try to find out the best linear fit to the data. The errors or residuals (the differences between the observed and predicted values) should be independent of each other. There should be no systematic patterns or correlations among the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc561b1-fdb3-4f31-804d-64459589c793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5092587-2466-4d8f-afa7-8f7dc8df2450",
   "metadata": {},
   "source": [
    "Q3. ans\n",
    "\n",
    "The process of selecting λ involves evaluating the model's performance on different subsets of the data for different values of λ. Cross-validation is a widely used technique to get this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffc8c4-59bd-4916-bc0e-9dfac84022bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f081518e-f322-4616-aec3-46e045b2e837",
   "metadata": {},
   "source": [
    "Q4. ans\n",
    "\n",
    "Yes but only in the case when there are variables that are highly corelated to the input variable i,e when there is need to handle Multicollinearity we use. Ridge Regression has the ability to handle multicollinearity, which occurs when predictor variables are highly correlated. The regularization in Ridge Regression helps stabilize the coefficient estimates, making them less sensitive to collinear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3827f3-69ca-4d1c-920c-19d548bd9fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36fc62d6-46ee-4c64-9cd0-81b3457abc6a",
   "metadata": {},
   "source": [
    "Q5. ans\n",
    "\n",
    "Ridge Regression's regularization term (L2 penalty) helps stabilize the coefficient estimates when multicollinearity is present. The regularization term penalizes large coefficient values, which reduces the impact of individual predictors on the model. As a result, the coefficient estimates become more robust and less sensitive to small changes in the data, including small variations due to multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a256e7-3c73-45fd-be5f-f59e46f2cdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4b404ef-9e18-4502-98e5-cbcc59032f35",
   "metadata": {},
   "source": [
    "Q6. ans\n",
    "\n",
    "Yes, Ridge Regression can handle both categorical and continuous independent variables, making it a versatile linear regression technique. Ridge Regression, like ordinary least squares (OLS) regression, can accommodate a mix of different types of predictors, including numerical (continuous) and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3649c825-2d82-418c-a6af-8d6bdb504359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "418b97c3-d695-4dde-843c-6658212d5e1d",
   "metadata": {},
   "source": [
    "Q7. ans\n",
    "\n",
    "Ridge Regression introduces a regularization term (L2 penalty) to the cost function, which penalizes large coefficient values. As a result, the coefficients in Ridge Regression are typically smaller than the coefficients obtained from OLS regression. interpreting the coefficients in Ridge Regression involves considering both the direction and magnitude of the coefficients, as well as the impact of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b0f68-6807-4685-8c0c-d05123331252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fce5ba5-3f76-41e8-88d7-f5d5cc93fe47",
   "metadata": {},
   "source": [
    "Q8. ans\n",
    "\n",
    "Yes, Ridge Regression can be used for time-series data analysis, but it requires some modifications to account for the temporal nature of the data. \n",
    "The Time-series data presents unique challenges due to its temporal dependencies, autocorrelation, and potential seasonality. Ridge Regression can be adapted for time-series analysis using a method known as \"Ridge Regression with Autoregressive Integrated Moving Average (ARIMA) Errors\" or \"Ridge ARIMA.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe7292-0dea-463f-80e0-8a53091297f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
